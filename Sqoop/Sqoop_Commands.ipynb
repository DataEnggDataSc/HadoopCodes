{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sqoop Commands.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCYy80Tsbx_G",
        "colab_type": "text"
      },
      "source": [
        "## Basic sqoop import Command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4uK_i-Rbekn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table user \\\n",
        "--target-dir sqoop_import_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUo_XgHHbiLC",
        "colab_type": "text"
      },
      "source": [
        "1. Target-dir must not exist beforehand. If it exists, the command while fail with the same error. \n",
        "2. This command will create the directory while sqooping, then would put the table data inside this directory as part files. \n",
        "3. There would be no subfolder created inside target-dir with the table name, datafiles would be put directly inside target-dir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJncxBbih8NF",
        "colab_type": "text"
      },
      "source": [
        "If target-dir is not provided, sqoop will create a directory in the current working directory with the same name as table name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIdkGVmeb9jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table user"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWOFkEg6cJl1",
        "colab_type": "text"
      },
      "source": [
        "## Controlling Parellelism\n",
        "\n",
        "1. To leverage parallelism, we need to provide number of mappers in the import command. There would be that many parallel imports executed, as mentioned in the --num-mappers parameter. \n",
        "2. That many number of part files would get created as there are number of mappers mentioned in the sqoop import command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLaTanRDcLIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table user \\\n",
        "--target-dir sqoop_import_dir \\\n",
        "--num-mappers 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF2q1vTbhuGa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "3. When performing parallel imports, Sqoop needs a criterion by which it can split the workload. Sqoop uses a splitting column to split the workload. By default, Sqoop will identify the primary key column (if present) in a table and use it as the splitting column. The low and high values for the splitting column are retrieved from the database, and the map tasks operate on evenly-sized components of the total range. For example, if you had a table with a primary key column of id whose minimum value was 0 and maximum value was 1000, and Sqoop was directed to use 4 tasks, Sqoop would run four processes which each execute SQL statements of the form ```SELECT * FROM sometable WHERE id >= lo AND id < hi```, with (lo, hi) set to (0, 250), (250, 500), (500, 750), and (750, 1001) in the different tasks. **(In the above sqoop, user_id is the primary key of the table 'user', which has been used as splitting key as can be seen from the sqoop log printed on console)**\n",
        "\n",
        "4. If the actual values for the primary key are not uniformly distributed across its range, then this can result in unbalanced tasks. You should explicitly choose a different column with the --split-by argument. For example, --split-by employee_id. This is also applicable for sqooping from a table which does not have a primary key, hence split-by column has to be mentioned **(Below we are sqooping from a mysql table 'logs' which had the columns logid,logdate,jobname,stepname,status,error_message. None of these has been made primary key of the table. Hence we have to either mention a column with --split-by parameter, or perform a sequential import using '-m 1'. Else it will throw error -** *ERROR tool.ImportTool: Error during import: No primary key could be found for table logs. Please specify one with --split-by or perform a sequential import with '-m 1'.*\n",
        "\n",
        "5. The split-by column has to be an integer column. If a textual column is passed, it would throw error - *ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Generating splits for a textual index column allowed only\n",
        " in case of \"-Dorg.apache.sqoop.splitter.allow_text_splitter=true\" property passed as a parameter* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X32KusY4hvBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table logs \\\n",
        "--target-dir sqoop_import_dir \\\n",
        "--num-mappers 3 \\\n",
        "--split-by logid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Muls8kNHpF6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Warehouse-dir parameter\n",
        "\n",
        "**1. warehouse-dir may exist beforehand, if does not exist it will be created while sqooping**\n",
        "2. During sqooping, a subfolder would be created inside the warehouse-dir with same name as the table name\n",
        "3. Data would be put inside this subfolder as part files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkqO-Lb3IXg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table user \\\n",
        "--warehouse-dir sqoop_import_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVYWwR3AJBbd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Using Options Files to Pass Arguments\n",
        "\n",
        "1. When using Sqoop, the command line options that do not change from invocation to invocation can be put in an options file for convenience. An options file is a text file where each line identifies an option in the order that it appears otherwise on the command line.\n",
        "2. To specify an options file, simply create an options file in a convenient location and pass it to the command line via --options-file argument.\n",
        "\n",
        "import.txt \\\n",
        "import \\\n",
        "--connect \\\n",
        "jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username \\\n",
        "sqoopuser \\\n",
        "--password \\\n",
        "NHkkP876rp\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJkIQr7rJC5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop --options-file /home/dataenggdatascfreelance1247/import.txt --table user"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsOVZ07cMGIM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Secure way of supplying password to the database\n",
        "\n",
        "You should save the password in a file on the users home directory with 400 permissions and specify the path to that file using the --password-file argument, and is the preferred method of entering credentials. Sqoop will then read the password from the file and pass it to the MapReduce cluster using secure means with out exposing the password in the job configuration. The file containing the password can either be on the Local FS or HDFS\n",
        "\n",
        "password.txt \\\n",
        "NHkkP876rp\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if5JiQG_MOTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \n",
        "--password-file /home/dataenggdatascfreelance1247/password.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om0W5NX-QIMT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Selecting the Data to Import\n",
        "\n",
        "By default, all columns within a table are selected for import. You can select \n",
        "a subset of columns and control their ordering by using the --columns argument\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HExzzvbPRHPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table user \\\n",
        "--columns name,age,country"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCGheBgqOPMt",
        "colab_type": "text"
      },
      "source": [
        "You can append a WHERE clause to this with the --where argument. For example: --where \"id > 400\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abu2gL8lOQ23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table user \\\n",
        "--columns name,age,country \\\n",
        "--where \"user_id > 2\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShhmstWmRH9q",
        "colab_type": "text"
      },
      "source": [
        "### Free-form Query Imports\n",
        "\n",
        "1. Sqoop can also import the result set of an arbitrary SQL query. Instead of using the --table, --columns and --where arguments, you can specify a SQL statement with the --query argument.\n",
        "2. When importing a free-form query, you must specify a destination directory with --target-dir.\n",
        "3. When importing query results in parallel, you must specify --split-by.\n",
        "4. Must provide \\$CONDITIONS in the query irrespective of whether actually using a where clause in the query or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVau5bmRRK80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--query \"SELECT * FROM user where \\$CONDITIONS\" \\\n",
        "--target-dir sqoop_import_dir \\\n",
        "--split-by user_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hc7om2zTQ0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--query \"SELECT u.user_id,u.name,u.age,us.salary FROM user u JOIN user_salary us ON u.user_id = us.user_id WHERE u.user_id in (1,2) AND \\$CONDITIONS\" \\\n",
        "--num-mappers 1 \\\n",
        "--target-dir sqoop_import_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjZ0FuW2RMJK",
        "colab_type": "text"
      },
      "source": [
        "## Incremental Imports\n",
        "\n",
        "1. The following arguments control incremental imports: \n",
        "--check-column (col) : Specifies the column to be examined when determining which rows to import. (the column should not be of type CHAR/NCHAR/VARCHAR/VARNCHAR/ LONGVARCHAR/LONGNVARCHAR)\\\n",
        "--incremental (mode) : Specifies how Sqoop determines which rows are new. Legal values for mode include 'append' and 'lastmodified'.\\\n",
        "--last-value (value) : Specifies the maximum value of the check column from the previous import.\n",
        "\n",
        "2. You should specify 'append' mode when importing a table where new rows are continually being added with increasing row id values. You specify the column containing the row id with --check-column. Sqoop imports rows where the check column has a value greater than the one specified with --last-value (last-value will be that value which has been last imported from the table).\n",
        "\n",
        "3. You should use 'lastmodified' when rows of the source table may be updated, and each such update will set the value of a last-modified column to the current timestamp. Rows where the check column holds a timestamp more recent than the timestamp specified with --last-value are imported.\n",
        "\n",
        "4. If --last-value is noe supplied while importing, it will import all the data of the table.\n",
        "\n",
        "5. At the end of an incremental import, the value which should be specified as --last-value for a subsequent import is printed to the screen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8od0btjxW3y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table user \\\n",
        "--check-column user_id \\\n",
        "--incremental append \\\n",
        "--last-value 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sioh1uR0quZ_",
        "colab_type": "text"
      },
      "source": [
        "## File Formats\n",
        "\n",
        "1. Delimited text is the default import format. You can also specify it explicitly by using the --as-textfile argument, with delimiter characters between individual columns and rows. These delimiters may be commas, tabs, or other characters. \n",
        "2. As output line formatting arguments, we use --enclosed-by <char>, --escaped-by <char>, --fields-terminated-by <char>, --lines-terminated-by <char>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3lKJaJvEa1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table user \\\n",
        "--fields-terminated-by ',' \\\n",
        "--lines-terminated-by '\\n' \\\n",
        "--target-dir sqoop_import_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c214YI9rLPHq",
        "colab_type": "text"
      },
      "source": [
        "3. If unambiguous delimiters cannot be presented, then use enclosing and escaping characters. The combination of (optional) enclosing and escaping characters will allow unambiguous parsing of lines. For example, suppose one column of a dataset contained the following values:\\\n",
        "`Some string, with a comma. \\\n",
        "Another \"string with quotes\"`\\\n",
        "The following arguments would provide delimiters which can be unambiguously parsed:\\\n",
        "`sqoop import --fields-terminated-by , --escaped-by \\\\ --enclosed-by '\\\"' ...`\\\n",
        "The result of the above arguments applied to the above dataset would be:\\\n",
        "`\"Some string, with a comma.\",\"1\",\"2\",\"3\"... \\\n",
        "\"Another \\\"string with quotes\\\"\",\"4\",\"5\",\"6\"...`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-QsHh5pLRvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table user \\\n",
        "--escaped-by \\\\ \\\n",
        "--enclosed-by '\\\"' \\\n",
        "--fields-terminated-by ',' \\\n",
        "--lines-terminated-by '\\n' \\\n",
        "--target-dir sqoop_import_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmslHpy4JQQC",
        "colab_type": "text"
      },
      "source": [
        "4. Delimited text is appropriate for most non-binary data types. It also readily supports further manipulation by other tools, such as Hive.\n",
        "5. SequenceFiles are a binary format that store individual records in custom record-specific data types. This format supports exact storage of all data in binary representations, and is appropriate for storing binary data (for example, VARBINARY columns)\n",
        "6. Reading from SequenceFiles is higher-performance than reading from text files, as records do not need to be parsed.\n",
        "7. Avro data files are a compact, efficient binary format that provides interoperability with applications written in other programming languages. Avro also supports versioning, so that when, e.g., columns are added or removed from a table, previously imported data files can be processed along with new ones.\n",
        "8. The parameters used are, --as-avrodatafile, --as-sequencefile, --as-textfile, --as-parquetfile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmfdyoXHHUQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table user \\\n",
        "--as-sequencefile \\\n",
        "--target-dir sqoop_import_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3TY94nsHdT3",
        "colab_type": "text"
      },
      "source": [
        "# Importing Data Into Hive\n",
        "\n",
        "1. Sqoop’s import tool’s main function is to upload your data into files in HDFS. If you have a Hive metastore associated with your HDFS cluster, Sqoop can also import the data into Hive by generating and executing a CREATE TABLE statement to define the data’s layout in Hive.\n",
        "2. If the Hive table already exists, you can specify the --hive-overwrite option to indicate that existing table in hive must be replaced. \n",
        "3. After your data is imported into HDFS, Sqoop will generate a Hive script containing a CREATE TABLE operation defining your columns using Hive’s types, and a LOAD DATA INPATH statement to move the data files into Hive’s warehouse directory.\n",
        "4. Sqoop will pass the field and record delimiters through to Hive. If you do not set any delimiters and do use --hive-import, the field delimiter will be set to ^A and the record delimiter will be set to \\n to be consistent with Hive’s defaults.\n",
        "5. The table name used in Hive is, by default, the same as that of the source table. You can control the output table name with the --hive-table option.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mksMcbcULnsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table user \\\n",
        "--hive-import \\\n",
        "--hive-table user_hive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN4WXscFNkEy",
        "colab_type": "text"
      },
      "source": [
        "6. Hive can put data into partitions for more efficient query performance. You can tell a Sqoop job to import data for Hive into a particular partition by specifying the --hive-partition-key and --hive-partition-value arguments. The partition value must be a string. Please see the Hive documentation for more details on partitioning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBA951qbNleB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table user \\\n",
        "--hive-import \\\n",
        "--hive-table user_hive \\\n",
        "--hive-partition-key user_id \\\n",
        "--hive-partition-value 5\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHiftHUqM6Hf",
        "colab_type": "text"
      },
      "source": [
        "7. Hive will have problems using Sqoop-imported data if your database’s rows contain string fields that have Hive’s default row delimiters (\\n and \\r characters) or column delimiters (\\01 characters) present in them. You can use the --hive-drop-import-delims option to drop those characters on import to give Hive-compatible text data. Alternatively, you can use the --hive-delims-replacement option to replace those characters with a user-defined string on import to give Hive-compatible text data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPtme4B2M7Ja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table user \\\n",
        "--hive-import \\\n",
        "--hive-table user_hive \\\n",
        "--hive-drop-import-delims"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YnpBrNthJxo",
        "colab_type": "text"
      },
      "source": [
        "# Importing Data Into HBase\n",
        "\n",
        "1. Each row of the input table will be transformed into an HBase Put operation to a row of the output table.\n",
        "2. The key for each row is taken from a column of the input. By default Sqoop will use the split-by column as the row key column. If that is not specified, it will try to identify the primary key column, if any, of the source table. You can manually specify the row key column with --hbase-row-key\n",
        "3. Each output column will be placed in the same column family, which must be specified with --column-family\n",
        "4. If the target table and column family do not exist, the Sqoop job will exit with an error. You should create the target table and column family before running an import\n",
        "5. If you specify --hbase-create-table, Sqoop will create the target table and column family if they do not exist.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYNjkHSAhJUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table user \\\n",
        "--hbase-table user_hbase \\\n",
        "--hbase-row-key user_id \\\n",
        "--column-family user_col_family \\\n",
        "--hbase-create-table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpV-X5YVtEof",
        "colab_type": "text"
      },
      "source": [
        "# Sqoop import-all-tables\n",
        "\n",
        "1. Data from each table is stored in a separate directory in HDFS.\n",
        "2. For the import-all-tables tool to be useful, the following conditions must be met:\n",
        "\n",
        "  - Each table must have a single-column primary key or --autoreset-to-one-mapper option must be used.\n",
        "  - You must intend to import all columns of each table.\n",
        "  - You must not intend to use non-default splitting column, nor impose any conditions via a WHERE clause.\n",
        "\n",
        "3. --table, --split-by, --columns, and --where arguments are invalid for sqoop-import-all-tables.\n",
        "4. --target-dir option can not be used, need to use --warehouse-dir option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVD1aN8Otyi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop import-all-tables \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--warehouse-dir sqoop_import_dir \\\n",
        "--autoreset-to-one-mapper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32YrG0LuwhzW",
        "colab_type": "text"
      },
      "source": [
        "# Sqoop export\n",
        "\n",
        "1. The target table of sqoop export must already exist in the database.\n",
        "2. By default, all columns within a table are selected for export. You can select a subset of columns and control their ordering by using the --columns argument. This should include a comma-delimited list of columns to export. For example: --columns \"col1,col2,col3\". Note that columns that are not included in the --columns parameter need to have either defined default value or allow NULL values. Otherwise your database will reject the imported data which in turn will make Sqoop job fail.\n",
        "3. Since Sqoop breaks down export process into multiple transactions, it is possible that a failed export job may result in partial data being committed to the database. This can further lead to subsequent jobs failing due to insert collisions in some cases, or lead to duplicated data in others. You can overcome this problem by specifying a staging table via the --staging-table option which acts as an auxiliary table that is used to stage exported data. The staged data is finally moved to the destination table in a single transaction.\n",
        "4. In order to use the staging facility, you must create the staging table prior to running the export job. This table must be structurally identical to the target table. This table should either be empty before the export job runs, or the --clear-staging-table option must be specified. If the staging table contains data and the --clear-staging-table option is specified, Sqoop will delete all of the data before starting the export job."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzPuJqz_wtVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop export \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--export-dir sqoop_import_dir \\\n",
        "--table user_export"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_UkKrHU1G10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop export \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--staging-table user_export_staging \\\n",
        "--clear-staging-table \\\n",
        "--table user_export \\\n",
        "--export-dir sqoop_import_dir\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsqfNWX92GKc",
        "colab_type": "text"
      },
      "source": [
        "# Inserts vs. Updates\n",
        "\n",
        "1. By default, sqoop-export appends new rows to a table; each input record is transformed into an INSERT statement that adds a row to the target database table. \n",
        "2. If you specify the --update-key argument, Sqoop will instead modify an existing dataset in the database. Each input record is treated as an UPDATE statement that modifies an existing row. The row a statement modifies is determined by the column name(s) specified with --update-key\n",
        "3. If an UPDATE statement modifies no rows, this is not considered an error; the export will silently continue. \n",
        "4. Depending on the target database, you may also specify the --update-mode argument with allowinsert mode if you want to update rows if they exist in the database already or insert rows if they do not exist yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_I1SWE02H0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop export \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--export-dir sqoop_import_dir \\\n",
        "--table user_export \\\n",
        "--update-key user_id \\\n",
        "--update-mode allowinsert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh0Fdd7R4asT",
        "colab_type": "text"
      },
      "source": [
        "# Saved Sqoop Jobs\n",
        "\n",
        "1. Imports and exports can be repeatedly performed by issuing the same command multiple times. Especially when using the incremental import capability, this is an expected scenario. Sqoop allows you to define saved jobs which make this process easier. A saved job records the configuration information required to execute a Sqoop command at a later time.\n",
        "2. Saved jobs remember the parameters used to specify a job, so they can be re-executed by invoking the job by its handle. If a saved job is configured to perform an incremental import, state regarding the most recently imported rows is updated in the saved job to allow the job to continually import only the newest rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh8dLNPA5Sgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop job --create myjob -- import \\\n",
        "--connect jdbc:mysql://cxln2.c.thelab-240901.internal/sqoopex \\\n",
        "--username sqoopuser \\\n",
        "--password NHkkP876rp \\\n",
        "--table user \\\n",
        "--target-dir sqoop_import_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH8YXJhg56vt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop job --list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukY94y6I57hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop job --show myjob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5M8I1885-t1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop job --exec myjob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl7f72Dx6LaG",
        "colab_type": "text"
      },
      "source": [
        "The exec action allows you to override arguments of the saved job by supplying them after a --. For example, if the database were changed to require a username, we could specify the username and password with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPSegW8G6K08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqoop job --exec myjob -- --username someuser -P"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEiOboSZ6P9S",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}