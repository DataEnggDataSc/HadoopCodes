{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hive Commands.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnczAd3EPnpH",
        "colab_type": "text"
      },
      "source": [
        "## Create Database Statement\n",
        "\n",
        "A database in Hive is a namespace or a collection of tables.\n",
        "\n",
        "Syntax - `CREATE DATABASE|SCHEMA [IF NOT EXISTS] <database name>`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuiVnSi4M_hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CREATE DATABASE IF NOT EXISTS dataenggdatascfreelance1247_db;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r733gTgQP8fa",
        "colab_type": "text"
      },
      "source": [
        "## Drop Database Statement\n",
        "\n",
        "Drops all the tables and deletes the database\n",
        "\n",
        "\n",
        "Syntax- `DROP DATABASE StatementDROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK8BJvrMRa-s",
        "colab_type": "text"
      },
      "source": [
        "### Drop database without table or Empty Database:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH4Kw3riQXsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DROP DATABASE IF EXISTS database_name;\n",
        "\n",
        "DROP SCHEMA database_name;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb--p6eTRcec",
        "colab_type": "text"
      },
      "source": [
        "### Drop database with tables:\n",
        "\n",
        "- The following query drops the database using CASCADE. It means dropping respective tables before dropping the database.\n",
        "\n",
        "- By default, the mode is RESTRICT which blocks the deletion of database if it holds tables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV6AbvdIQtz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DROP DATABASE database_name CASCADE;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3RhlZhxsb0v",
        "colab_type": "text"
      },
      "source": [
        "## Create Table Statement\n",
        "\n",
        "CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name\\\n",
        " (col_name data_type [column_constraint_specification] [COMMENT col_comment],\\\n",
        "  col_name data_type [column_constraint_specification] [COMMENT col_comment],.. constraint_specification)\\\n",
        "COMMENT table_comment\\\n",
        "PARTITIONED BY (col_name data_type COMMENT col_comment, ...)\\\n",
        "CLUSTERED BY (col_name, col_name, ...)\\\n",
        "SORTED BY (col_name [ASC|DESC], ...) INTO num_buckets BUCKETS\\\n",
        "SKEWED BY (col_name, col_name, ...)\\\n",
        "ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)\\\n",
        "STORED AS DIRECTORIES\\\n",
        "ROW FORMAT row_format\\\n",
        "STORED AS file_format\\\n",
        "LOCATION hdfs_path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4b9Zu0BRiXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE dataenggdatascfreelance1247_db;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8615v1Fsjx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CREATE TABLE IF NOT EXISTS employee \n",
        "(\n",
        "eid int COMMENT 'Employee ID', \n",
        "name string COMMENT 'Employee Name',\n",
        "salary float COMMENT 'Employee Salary', \n",
        "designation String COMMENT 'Employee Designation')\n",
        "COMMENT 'Employee details'\n",
        "ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' LINES TERMINATED BY '\\n'\n",
        "STORED AS TEXTFILE;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV2Z0hawvTHK",
        "colab_type": "text"
      },
      "source": [
        "**file_format:**\n",
        "  - SEQUENCEFILE\n",
        "  - TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)\n",
        "  - RCFILE      -- (Note: Available in Hive 0.6.0 and later)\n",
        "  - ORC         -- (Note: Available in Hive 0.11.0 and later)\n",
        "  - PARQUET     -- (Note: Available in Hive 0.13.0 and later)\n",
        "  - AVRO        -- (Note: Available in Hive 0.14.0 and later)\n",
        "  - JSONFILE    -- (Note: Available in Hive 4.0.0 and later)\n",
        "\n",
        "**column_constraint_specification:**\n",
        "  - PRIMARY KEY\n",
        "  - UNIQUE\n",
        "  - NOT NULL\n",
        "  - DEFAULT [default_value]\n",
        "  - CHECK  [check_expression] \n",
        "  - ENABLE|DISABLE \n",
        "  \n",
        "**constraint_specification:**\n",
        "  - PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE RELY/NORELY\n",
        "  - PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE RELY/NORELY\n",
        "  - CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE \n",
        "  - CONSTRAINT constraint_name UNIQUE (col_name, ...) DISABLE NOVALIDATE RELY/NORELY\n",
        "  - CONSTRAINT constraint_name CHECK [check_expression] ENABLE|DISABLE NOVALIDATE RELY/NORELY  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uivr-3N9wa_p",
        "colab_type": "text"
      },
      "source": [
        "## Load Data Statement\n",
        "\n",
        "In Hive, we can insert data using the LOAD DATA statement.\n",
        "\n",
        "While inserting data into Hive, it is better to use LOAD DATA to store bulk records. There are two ways to load data: one is **from local file system** and second is **from Hadoop file system**.\n",
        "\n",
        "The syntax for load data is as follows:\\\n",
        "`LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename \n",
        "[PARTITION (partcol1=val1, partcol2=val2 ...)]`\n",
        "\n",
        "- LOCAL is identifier to specify the local path. It is optional.\n",
        "- OVERWRITE is optional to overwrite the data in the table.\n",
        "- PARTITION is optional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZc2jfaswCtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOAD DATA LOCAL INPATH '/home/dataenggdatascfreelance1247/hive_data/sample.txt' INTO TABLE employee;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuEG3JIqxcd8",
        "colab_type": "text"
      },
      "source": [
        "## Alter Table\n",
        "\n",
        "ALTER TABLE name RENAME TO new_name\\\n",
        "ALTER TABLE name ADD COLUMNS (col_spec[, col_spec ...])\\\n",
        "ALTER TABLE name DROP [COLUMN] column_name\\\n",
        "ALTER TABLE name CHANGE column_name new_name new_type\\\n",
        "ALTER TABLE name REPLACE COLUMNS (col_spec[, col_spec ...])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEp9hMKrA4zZ",
        "colab_type": "text"
      },
      "source": [
        "Rename table employee to table emp "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVRR3DtE_yA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ALTER TABLE employee RENAME TO emp;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAymyfBXAaZ5",
        "colab_type": "text"
      },
      "source": [
        "Alter table employee to change column 'name'(string) to column 'ename'(string)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeUwIzwWAK3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DESCRIBE employee;\n",
        "\n",
        "ALTER TABLE employee CHANGE name ename String;\n",
        "\n",
        "DESCRIBE employee;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toYpzFcxAqLt",
        "colab_type": "text"
      },
      "source": [
        "Alter Table employee to change column salary(float) to salary(double)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRFNBImIAyKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DESCRIBE employee;\n",
        "\n",
        "ALTER TABLE employee CHANGE salary salary Double;\n",
        "\n",
        "DESCRIBE employee;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iob5AVzzA-zy",
        "colab_type": "text"
      },
      "source": [
        "Add column named dept to the employee table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LXtwdvcBCFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DESCRIBE employee;\n",
        "\n",
        "ALTER TABLE employee ADD COLUMNS (dept STRING COMMENT 'Department name');\n",
        "\n",
        "DESCRIBE employee;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq0hPGN1BXt6",
        "colab_type": "text"
      },
      "source": [
        "Alter table employee to deletes all the columns from the employee table and replaces it with emp and name columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzNMDkfpBiEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DESCRIBE employee;\n",
        "\n",
        "ALTER TABLE employee REPLACE COLUMNS (empid Int, empname String);\n",
        "\n",
        "DESCRIBE employee;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9gmBuGZWzE8",
        "colab_type": "text"
      },
      "source": [
        "##Drop Table \n",
        "\n",
        "The syntax is as follows:\n",
        "\n",
        "`DROP TABLE [IF EXISTS] table_name;`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OlusWDoWz8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DROP TABLE IF EXISTS employee;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M75QoNE-hwP",
        "colab_type": "text"
      },
      "source": [
        "## Hive - Partitioning\n",
        "\n",
        "### What are the Hive Partitions\n",
        "Partitioning is a way of dividing a table into related parts based on the values of particular columns like date, city, and department. Each table in the hive can have one or more partition keys to identify a particular partition. Using partition it is easy to do queries on slices of the data.\n",
        "\n",
        "### Why is Partitioning Important\n",
        "- We know that the huge amount of data which is in the range of petabytes is getting stored in HDFS. So due to this, it becomes very difficult for Hadoop users to query this huge amount of data. \n",
        "- Hive was introduced to lower down this burden of data querying. Apache Hive converts the SQL queries into MapReduce jobs and then submits it to the Hadoop cluster. When we submit a SQL query, Hive read the entire data-set. So, it becomes inefficient to run MapReduce jobs over a large table. \n",
        "- This is resolved by creating partitions in tables. Apache Hive makes this job of implementing partitions very easy by creating partitions by its automatic partition scheme at the time of table creation.\n",
        "- In Partitioning method, all the table data is divided into multiple partitions. Each partition corresponds to a specific value(s) of partition column(s). It is kept as a sub-record inside the table’s record present in the HDFS. Therefore on querying a particular table, appropriate partition of the table is queried which contains the query value. Thus this decreases the I/O time required by the query. Hence increases the performance speed.\n",
        "\n",
        "### Partitioning Example\n",
        "A table named Tab1 contains employee data such as id, name, dept, and yoj (i.e. year of joining). Suppose you need to retrieve the details of all employees who joined in 2012. A query searches the whole table for the required information. However, if you partition the employee data with the year and store it in a separate file, it reduces the query processing time by searching only inside the partition for 2012.\\\n",
        "\n",
        "The following example shows how to partition a file and its data:\n",
        "\n",
        "The following file contains employeedata table.\n",
        "\n",
        "/tab1/employeedata/file1\n",
        "\n",
        "id, name, dept, yoj\\\n",
        "1, gopal, TP, 2012\\\n",
        "2, kiran, HR, 2012\\\n",
        "3, kaleel,SC, 2013\\\n",
        "4, Prasanth, SC, 2013\n",
        "\n",
        "The above data is partitioned into two files using year.\n",
        "\n",
        "/tab1/employeedata/2012/file2\n",
        "\n",
        "1, gopal, TP, 2012\\\n",
        "2, kiran, HR, 2012\n",
        "\n",
        "\n",
        "/tab1/employeedata/2013/file3\n",
        "\n",
        "3, kaleel,SC, 2013\\\n",
        "4, Prasanth, SC, 2013\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnZHXTC9CBMm",
        "colab_type": "text"
      },
      "source": [
        "### How to Create Partitions in Hive\n",
        "To create data partitioning in Hive following command is used-\n",
        "\n",
        "`CREATE TABLE table_name (column1 data_type, column2 data_type) PARTITIONED BY (partition1 data_type, partition2 data_type,….);`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ysN1-jGBscD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE dataenggdatascfreelance1247_db;\n",
        "\n",
        "CREATE TABLE partitioned_table (id INT, name STRING, dept STRING, yoj INT) \n",
        "PARTITIONED BY (year STRING)\n",
        "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n';\n",
        "\n",
        "LOAD DATA LOCAL INPATH 'hive_data/file_partition1.txt' OVERWRITE INTO TABLE partitioned_table PARTITION (year='2012');\n",
        "LOAD DATA LOCAL INPATH 'hive_data/file_partition2.txt' OVERWRITE INTO TABLE partitioned_table PARTITION (year='2013');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmNvmZunFebT",
        "colab_type": "text"
      },
      "source": [
        "### Types of Hive Partitioning\n",
        "\n",
        "There are two types of Partitioning in Apache Hive-\n",
        "\n",
        "- Static Partitioning\n",
        "- Dynamic Partitioning\n",
        "\n",
        "**Hive Static Partitioning**\n",
        "- Insert input data files individually into a partition table is Static Partition.\n",
        "- Usually when loading files (big files) into Hive tables static partitions are preferred.\n",
        "- Static Partition saves your time in loading data compared to dynamic partition.\n",
        "- You “statically” add a partition in the table and move the file into the partition of the table.\n",
        "- We can alter the partition in the static partition.\n",
        "- You can get the partition column value from the filename, day of date etc without reading the whole big file.\n",
        "- ***If you want to use the Static partition in the hive you should set property set hive.mapred.mode = strict This property set by default in hive-site.xml***\n",
        "- Static partition is in Strict Mode.\n",
        "- You should use where clause to use limit in the static partition.\n",
        "- You can perform Static partition on Hive Manage table or external table.\n",
        "\n",
        "**Hive Dynamic Partitioning**\n",
        "- Single insert to partition table is known as a dynamic partition.\n",
        "- Usually, dynamic partition loads the data from the non-partitioned table.\n",
        "- Dynamic Partition takes more time in loading data compared to static partition.\n",
        "- When you have large data stored in a table then the Dynamic partition is suitable.\n",
        "- If you want to partition a number of columns but you don’t know how many columns then also dynamic partition is suitable.\n",
        "- Dynamic partition there is no required where clause to use limit.\n",
        "- we can’t perform alter on the Dynamic partition.\n",
        "- You can perform dynamic partition on hive external table and managed table.\n",
        "- ***If you want to use the Dynamic partition in the hive then the mode is in non-strict mode.***\n",
        "- Here are Hive dynamic partition properties you should allow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7UFluYoGH2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}