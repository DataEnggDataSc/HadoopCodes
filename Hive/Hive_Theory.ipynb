{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hive Theory.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zif2Y5xlc3n",
        "colab_type": "text"
      },
      "source": [
        "Hive is a data warehouse infrastructure tool to process structured data in Hadoop. It resides on top of Hadoop to summarize Big Data, and makes querying and analyzing easy.\n",
        "\n",
        "Initially Hive was developed by Facebook, later the Apache Software Foundation took it up and developed it further as an open source under the name Apache Hive. It is used by different companies. For example, Amazon uses it in Amazon Elastic MapReduce and Microsoft uses in Azure HDInsight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8sFPrlXrk7c",
        "colab_type": "text"
      },
      "source": [
        "**Hive is not**\n",
        "- A relational database\n",
        "- A design for OnLine Transaction Processing (OLTP)\n",
        "- A language for real-time queries and row-level updates\n",
        "\n",
        "**Features of Hive**\n",
        "- It stores schema in a database and processed data into HDFS.\n",
        "- It is designed for OLAP.\n",
        "- It provides SQL type language for querying called HiveQL or HQL.\n",
        "- It is familiar, fast, scalable, and extensible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek22H0p0xirT",
        "colab_type": "text"
      },
      "source": [
        "## Architecture of Hive\n",
        "\n",
        "![diagram from Tutorialspoint](https://www.tutorialspoint.com/hive/images/hive_architecture.jpg)\n",
        "\n",
        "This component diagram contains different units. The following table describes each unit:\n",
        "\n",
        "**User Interface** - Hive is a data warehouse infrastructure software that can create interaction between user and HDFS. The user interfaces that Hive supports are Hive Web UI, Hive command line, and Hive HD Insight (In Windows server).\n",
        "\n",
        "**Meta Store** - Hive chooses respective database servers to store the schema or Metadata of tables, databases, columns in a table, their data types, and HDFS mapping.\n",
        "\n",
        "\n",
        "**HiveQL Process Engine** -\tHiveQL is similar to SQL for querying on schema info on the Metastore. It is one of the replacements of traditional approach for MapReduce program. Instead of writing MapReduce program in Java, we can write a query for MapReduce job and process it.\n",
        "\n",
        "\n",
        "**Execution Engine** - The conjunction part of HiveQL process Engine and MapReduce is Hive Execution Engine. Execution engine processes the query and generates results as same as MapReduce results. It uses the flavor of MapReduce.\n",
        "HDFS or HBASE\tHadoop distributed file system or HBASE are the data storage techniques to store data into file system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj67Mi6Oy5Na",
        "colab_type": "text"
      },
      "source": [
        "## Working of Hive\n",
        "\n",
        "![diagram from Tutorialspoint](https://www.tutorialspoint.com/hive/images/how_hive_works.jpg)\n",
        "\n",
        "**Step 1: Execute Query** - The Hive interface such as Command Line or Web UI sends query to Driver (any database driver such as JDBC, ODBC, etc.) to execute.\n",
        "\n",
        "**Step 2: Get Plan** - The driver takes the help of query compiler that parses the query to check the syntax and query plan or the requirement of query.\n",
        "\n",
        "**Step 3: Get Metadata** - The compiler sends metadata request to Metastore (any database).\n",
        "\n",
        "**Step 4:\tSend Metadata** - Metastore sends metadata as a response to the compiler.\n",
        "\n",
        "**Step 5: Send Plan** - The compiler checks the requirement and resends the plan to the driver. Up to here, the parsing and compiling of a query is complete.\n",
        "\n",
        "**Step 6: Execute Plan** - The driver sends the execute plan to the execution engine.\n",
        "\n",
        "**Step 7: Execute Job** - Internally, the process of execution job is a MapReduce job. The execution engine sends the job to JobTracker, which is in Name node and it assigns this job to TaskTracker, which is in Data node. Here, the query executes MapReduce job.\n",
        "\n",
        "**7.1\tMetadata Ops** -Meanwhile in execution, the execution engine can execute metadata operations with Metastore.\n",
        "\n",
        "**Step 8: Fetch Result** -The execution engine receives the results from Data nodes.\n",
        "\n",
        "**Step 9: Send Results** - The execution engine sends those resultant values to the driver.\n",
        "\n",
        "**Step 10: Send Results** - The driver sends the results to Hive Interfaces.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyhT1mjxBnM7",
        "colab_type": "text"
      },
      "source": [
        "## Configuring Metastore of Hive\n",
        "\n",
        "Configuring Metastore means specifying to Hive where the database is stored. You can do this by editing the **hive-site.xml** file, which is in the **$HIVE_HOME/conf** directory. \n",
        "\n",
        "- To get the details of the database where metastore is created, need to search for property **javax.jdo.option.ConnectionURL**\n",
        "\n",
        "- To get the hive directory inside HDFS, need to search for property **hive.metastore.warehouse.dir**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zG5_w-IjYc",
        "colab_type": "text"
      },
      "source": [
        "## Hive - Data Types\n",
        "\n",
        "All the data types in Hive are classified into four types, given as follows:\n",
        "\n",
        "- Column Types\n",
        "- Complex Types\n",
        "\n",
        "**Column Types**  \n",
        "\n",
        "Integral Types - TINYINT, SMALLINT, INT, BIGINT\\\n",
        "\n",
        "String Types - VARCHAR, CHAR\\\n",
        "\n",
        "Timestamp - Supports format `YYYY-MM-DD HH:MM:SS.fffffffff` and format `yyyy-mm-dd hh:mm:ss.ffffffffff`\n",
        "\n",
        "Dates - Supports in the format `YYYY-MM-DD`\n",
        "\n",
        "Decimals - Float datatype of the format `DECIMAL(precision, scale)`\n",
        "\n",
        "Union Types - Union is a collection of heterogeneous data types. You can create an instance using create union.\n",
        "`UNIONTYPE<int, double, array<string>, struct<a:int,b:string>>`\n",
        "\n",
        "**Complex Types**  \n",
        "\n",
        "Arrays - Arrays in Hive are used the same way they are used in Java. Syntax: `ARRAY<data_type>`\n",
        "\n",
        "Maps - Maps in Hive are similar to Java Maps. Syntax: `MAP<primitive_type, data_type>`\n",
        "\n",
        "Structs - Structs in Hive is similar to using complex data with comment. Syntax: `STRUCT<col_name : data_type [COMMENT col_comment], ...>`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETZZpKXYMZK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}