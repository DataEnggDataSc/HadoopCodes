{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pig Theory.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymOHl7xbYLMs",
        "colab_type": "text"
      },
      "source": [
        "1. Apache Pig is an abstraction over MapReduce. It is a tool/platform which is used to analyze larger sets of data representing them as data flows. Pig is generally used with Hadoop; we can perform all the data manipulation operations in Hadoop using Apache Pig.\n",
        "\n",
        "2. To write data analysis programs, Pig provides a high-level language known as Pig Latin. This language provides various operators using which programmers can develop their own functions for reading, writing, and processing data.\n",
        "\n",
        "3. To analyze data using Apache Pig, programmers need to write scripts using Pig Latin language. All these scripts are internally converted to Map and Reduce tasks. Apache Pig has a component known as Pig Engine that accepts the Pig Latin scripts as input and converts those scripts into MapReduce jobs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5x-uV7FYi2V",
        "colab_type": "text"
      },
      "source": [
        "## Why Do We Need Apache Pig?\n",
        "\n",
        "1. Using Pig Latin, programmers can perform MapReduce tasks easily without having to type complex codes in Java.\n",
        "\n",
        "2. Apache Pig uses multi-query approach, thereby reducing the length of codes. For example, an operation that would require you to type 200 lines of code (LoC) in Java can be easily done by typing as less as just 10 LoC in Apache Pig. Ultimately Apache Pig reduces the development time by almost 16 times.\n",
        "\n",
        "3. Pig Latin is SQL-like language and it is easy to learn Apache Pig when you are familiar with SQL.\n",
        "\n",
        "4. Apache Pig provides many built-in operators to support data operations like joins, filters, ordering, etc. In addition, it also provides nested data types like tuples, bags, and maps that are missing from MapReduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQFFMIaXg64C",
        "colab_type": "text"
      },
      "source": [
        "## Apache Pig Vs MapReduce\n",
        "\n",
        "1. Apache Pig is a data flow language.\tMapReduce is a data processing paradigm.\n",
        "2. It is a high level language.\tMapReduce is low level and rigid.\n",
        "3. Performing a Join operation in Apache Pig is pretty simple.\tIt is quite difficult in MapReduce to perform a Join operation between datasets.\n",
        "4. Any novice programmer with a basic knowledge of SQL can work conveniently with Apache Pig.\tExposure to Java is must to work with MapReduce.\n",
        "5. Apache Pig uses multi-query approach, thereby reducing the length of the codes to a great extent.\tMapReduce will require almost 20 times more the number of lines to perform the same task.\n",
        "6. There is no need for compilation. On execution, every Apache Pig operator is converted internally into a MapReduce job.\tMapReduce jobs have a long compilation process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oslaGEWbhTi1",
        "colab_type": "text"
      },
      "source": [
        "## Apache Pig Vs Hive\n",
        "Both Apache Pig and Hive are used to create MapReduce jobs. And in some cases, Hive operates on HDFS in a similar way Apache Pig does. Still there are the following differences - \n",
        "\n",
        "1. Apache Pig uses a language called Pig Latin. It was originally created at Yahoo.\tHive uses a language called HiveQL. It was originally created at Facebook.\n",
        "2. Pig Latin is a data flow language.\tHiveQL is a query processing language.\n",
        "3. Pig Latin is a procedural language and it fits in pipeline paradigm.\tHiveQL is a declarative language.\n",
        "4. Apache Pig can handle structured, unstructured, and semi-structured data.\tHive is mostly for structured data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8LVMsD0k_J6",
        "colab_type": "text"
      },
      "source": [
        "## Pig Latin, a Parallel Data Flow Language\n",
        "\n",
        "Pig Latin is a data flow language. This means it allows users to describe how data from one or more inputs should be read, processed, and then stored to one or more outputs in parallel. These data flows can be simple linear flows, or complex workflows that include points where multiple inputs are joined and where data is split into multiple streams to be processed by different operators. To be mathematically precise, a Pig Latin script describes a directed acyclic graph (DAG), where the edges are data flows and the nodes are operators that process the data.\n",
        "\n",
        "This means that Pig Latin looks different from many of the programming languages you may have seen. There are no if statements or for loops in Pig Latin. This is because traditional procedural and object-oriented programming languages describe control flow, and data flow is a side effect of the program. Pig Latin instead focuses on data flow. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeA-L2skobla",
        "colab_type": "text"
      },
      "source": [
        "## Comparing Query and Data Flow Languages\n",
        "\n",
        "After a cursory look, people often say that Pig Latin is a procedural version of SQL. Although there are certainly similarities, there are more differences. SQL is a query language. Its focus is to allow users to form queries. It lets users describe what question they want answered, but not how they want it answered. In Pig Latin, on the other hand, the user describes exactly how to process the input data.\n",
        "\n",
        "Another major difference is that SQL is oriented around answering one question. When users want to do several data operations together, they must either write separate queries, storing the intermediate data into temporary tables, or use subqueries inside the query to do the earlier steps of the processing. However, many SQL users find subqueries confusing and difficult to form properly. Also, using subqueries creates an inside-out design where the first step in the data pipeline is the innermost query.\n",
        "\n",
        "Pig, however, is designed with a long series of data operations in mind, so there is no need to write the data pipeline in an inverted set of subqueries or to worry about storing data in temporary tables. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6vjwm20pjNH",
        "colab_type": "text"
      },
      "source": [
        "## Applications of Apache Pig\n",
        "\n",
        "Apache Pig is generally used by data scientists for performing tasks involving ad-hoc processing and quick prototyping. Apache Pig is used −\n",
        "\n",
        "- To process huge data sources such as web logs.\n",
        "- To perform data processing for search platforms.\n",
        "- To process time sensitive data loads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vttq-1i6rG7u",
        "colab_type": "text"
      },
      "source": [
        "## Apache Pig Architecture\n",
        "\n",
        "To perform a particular task Programmers using Pig, programmers need to write a Pig script using the Pig Latin language, and execute them using any of the execution mechanisms (Grunt Shell, UDFs, Embedded). After execution, these scripts will go through a series of transformations applied by the Pig Framework, to produce the desired output.\n",
        "\n",
        "Internally, Apache Pig converts these scripts into a series of MapReduce jobs, and thus, it makes the programmer’s job easy. The architecture of Apache Pig is shown below.\n",
        "\n",
        "![alt text](https://www.tutorialspoint.com/apache_pig/images/apache_pig_architecture.jpg)\n",
        "\n",
        "### Parser\n",
        "Initially the Pig Scripts are handled by the Parser. It checks the syntax of the script, does type checking, and other miscellaneous checks. The output of the parser will be a DAG (directed acyclic graph), which represents the Pig Latin statements and logical operators. In the DAG, the logical operators of the script are represented as the nodes and the data flows are represented as edges.\n",
        "\n",
        "### Optimizer\n",
        "The logical plan (DAG) is passed to the logical optimizer, which carries out the logical optimizations such as projection and pushdown.\n",
        "\n",
        "### Compiler\n",
        "The compiler compiles the optimized logical plan into a series of MapReduce jobs.\n",
        "\n",
        "### Execution engine\n",
        "Finally the MapReduce jobs are submitted to Hadoop in a sorted order. Finally, these MapReduce jobs are executed on Hadoop producing the desired results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd2GcLxrsa1U",
        "colab_type": "text"
      },
      "source": [
        "## Pig Latin Data Model\n",
        "\n",
        "### Atom\n",
        "Any single value in Pig Latin, irrespective of their data, type is known as an Atom. It is stored as string and can be used as string and number. int, long, float, double, chararray, and bytearray are the atomic values of Pig. A piece of data or a simple atomic value is known as a field.\n",
        "\n",
        "Example − ‘raja’ or ‘30’\n",
        "\n",
        "### Tuple\n",
        "A record that is formed by an ordered set of fields is known as a tuple, the fields can be of any type. A tuple is similar to a row in a table of RDBMS.\n",
        "\n",
        "Example − (Raja, 30)\n",
        "\n",
        "### Bag\n",
        "A bag is an unordered set of tuples. In other words, a collection of tuples (non-unique) is known as a bag. Each tuple can have any number of fields (flexible schema). A bag is represented by ‘{}’. **It is similar to a table in RDBMS, but unlike a table in RDBMS, it is not necessary that every tuple contain the same number of fields or that the fields in the same position (column) have the same type.**\n",
        "\n",
        "Example − {(Raja, 30), (Mohammad, 45)}\n",
        "\n",
        "A bag can be a field in a relation; in that context, it is known as inner bag.\n",
        "\n",
        "Example − {Raja, 30, {9848022338, raja@gmail.com,}}\n",
        "\n",
        "### Map\n",
        "A map (or data map) is a set of key-value pairs. **The key needs to be of type chararray and should be unique. The value might be of any type**. It is represented by ‘[]’\n",
        "\n",
        "Example − [name#Raja, age#30]\n",
        "\n",
        "### Relation\n",
        "A relation is a bag of tuples. The relations in Pig Latin are unordered (there is no guarantee that tuples are processed in any particular order)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqaOu0uisaUn",
        "colab_type": "text"
      },
      "source": [
        "## Apache Pig Execution Modes\n",
        "You can run Apache Pig in two modes, namely, Local Mode and HDFS mode.\n",
        "\n",
        "**Local Mode**\\\n",
        "In this mode, all the files are installed and run from your local host and local file system. There is no need of Hadoop or HDFS. This mode is generally used for testing purpose.\n",
        "\n",
        "Command to enter this mode : pig –x local\n",
        "\n",
        "**MapReduce Mode**\\\n",
        "MapReduce mode is where we load or process the data that exists in the Hadoop File System (HDFS) using Apache Pig. In this mode, whenever we execute the Pig Latin statements to process the data, a MapReduce job is invoked in the back-end to perform a particular operation on the data that exists in the HDFS\n",
        "\n",
        "Command to enter this mode : pig –x mapreduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8sNx3HsvWUY",
        "colab_type": "text"
      },
      "source": [
        "## Apache Pig Execution Mechanisms\n",
        "Apache Pig scripts can be executed in three ways, namely, interactive mode, batch mode, and embedded mode.\n",
        "\n",
        "**Interactive Mode (Grunt shell)** − You can run Apache Pig in interactive mode using the Grunt shell. In this shell, you can enter the Pig Latin statements and get the output (using Dump operator).\n",
        "\n",
        "**Batch Mode (Script)** − You can run Apache Pig in Batch mode by writing the Pig Latin script in a single file with .pig extension.\n",
        "\n",
        "**Embedded Mode (UDF)** − Apache Pig provides the provision of defining our own functions (User Defined Functions) in programming languages such as Java, and using them in our script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBbQ0P7orGJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOgNzvU4g31A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}